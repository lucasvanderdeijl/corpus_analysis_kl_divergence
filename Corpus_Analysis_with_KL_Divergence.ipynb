{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Analysis with  Kullbackâ€“Leibler divergence\n",
    "\n",
    "Author: Lucas van der Deijl, University of Amsterdam <br/>\n",
    "Version: 9 December 2020 <br/>\n",
    "Contact: l.a.vanderdeijl@uva.nl, www.lucasvanderdeijl.nl <br/>\n",
    "Project: 'Radical Rumours' (Funded by NWO 2017-2021) <br/>\n",
    "\n",
    "## Aim of this program\n",
    "\n",
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries\n",
    "\n",
    "First, the required libaries and resources need to be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # We're going to use the Python package scikit-learn to transform texts into vectors of TF-IDF values\n",
    "from sklearn.metrics.pairwise import cosine_similarity # scikut-learn also offers various ready-to-use methods for calculating metrics like cosine similarity\n",
    "from scipy.stats import entropy\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install missing libraries\n",
    "\n",
    "In case you got an error after the previous step because not all of the required modules are installed, you can uncomment (remove the '#') the relevant install-command below and run the code. Once the module is installed, run the block above again to import it before moving on to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for preprocessing and parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "  stopwords = open((\"Resources/stopwoorden.txt\"), 'rt', encoding='utf-8').read().split()\n",
    "  punct = punctuation\n",
    "  tokens = word_tokenize(doc)\n",
    "  lowercase_tokens = [token.lower() for token in tokens]\n",
    "  punct_and_stops_removed = \" \".join([token for token in lowercase_tokens if (token not in stopwords) and (token not in punct)]) \n",
    "  preprocessed_doc = punct_and_stops_removed\n",
    "  return(preprocessed_doc)\n",
    "\n",
    "def parse_corpus(corpus_location):\n",
    "    corpus = []\n",
    "    titles = []\n",
    "    for filename in os.listdir(corpus_location):\n",
    "        title = filename.split(\"_\")[2]\n",
    "        titles.append(title)\n",
    "        file = open((corpus_location + filename), 'rt', encoding='utf-8')\n",
    "        preprocessed_text = preprocess(file.read())\n",
    "        corpus.append(preprocessed_text)\n",
    "        file.close()\n",
    "    return(titles, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filepaths to corpora\n",
    "source_label = \"Descartes\"\n",
    "target_label = \"Spinoza\"\n",
    "\n",
    "path_to_corpusfolder = \"Corpus/\"\n",
    "#os.listdir(path_to_corpusfolder) # uncomment and run to check if your path is  correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess your corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_corpus = parse_corpus(path_to_corpusfolder + source_label + \"/\")\n",
    "target_corpus = parse_corpus(path_to_corpusfolder + target_label + \"/\")\n",
    "\n",
    "document_titles = source_corpus[0] + target_corpus[0]\n",
    "total_corpus = source_corpus[1] + target_corpus[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a term-document matrix with tfidf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(min_df=0) # set parameters for vectorization\n",
    "term_doc_matrix = vect.fit_transform(total_corpus)\n",
    "term_doc_matrix_array = term_doc_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a document-document matrix and compute KL divergence for both directions in each document pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_to_target_dict = {}\n",
    "target_to_source_dict = {}\n",
    "\n",
    "for first_counter, tfidf_array_source in enumerate(term_doc_matrix_array[:len(source_corpus[1])]):\n",
    "    source_index = first_counter\n",
    "    source_title = document_titles[source_index]\n",
    "    source_to_target_dict[source_title] = {}\n",
    "    \n",
    "    for second_counter, tfidf_array_target in enumerate(term_doc_matrix_array[len(source_corpus[1]):]): \n",
    "        target_index = len(source_corpus[1]) + second_counter\n",
    "        target_title = document_titles[target_index]\n",
    "        \n",
    "        no_zeroes_source = []\n",
    "        no_zeroes_target = []\n",
    "        for index in range(len(tfidf_array_source)):\n",
    "            if tfidf_array_source[index] != 0 and tfidf_array_target[index] != 0: \n",
    "                no_zeroes_source.append(tfidf_array_source[index])\n",
    "                no_zeroes_target.append(tfidf_array_target[index])\n",
    "\n",
    "        source_to_target_dict[source_title][target_title] = round(entropy(no_zeroes_target, qk=no_zeroes_source),2) \n",
    "        if target_title not in target_to_source_dict:\n",
    "            target_to_source_dict[target_title] = {}\n",
    "            target_to_source_dict[target_title][source_title] = round(entropy(no_zeroes_source, qk=no_zeroes_target),2)\n",
    "        else:\n",
    "            target_to_source_dict[target_title][source_title] = round(entropy(no_zeroes_source, qk=no_zeroes_target),2)\n",
    "            \n",
    "source_to_target_list_of_lists = [list(source_to_target_dict[key].values()) for key in source_to_target_dict]\n",
    "source_to_target_flattened = [value for sublist in source_to_target_list_of_lists for value in sublist if value != 0.0]\n",
    "\n",
    "target_to_source_list_of_lists = [list(target_to_source_dict[key].values()) for key in target_to_source_dict]\n",
    "target_to_source_flattened = [value for sublist in target_to_source_list_of_lists for value in sublist if value != 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the results as boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "fig1, ax1 = plt.subplots()\n",
    "plot_title = \"Surprise of reading \" + target_label + \" after \" + source_label + \" and vice versa\"\n",
    "ax1.set_title(plot_title)\n",
    "ax1.boxplot([source_to_target_flattened, target_to_source_flattened])\n",
    "ax1.set_xticklabels([target_label + \" after \" + source_label, source_label + \" after \" + target_label])\n",
    "ax1.set_ylim([0.0,2.5])\n",
    "#plt.savefig(\"Output/BOXPLOT.png\", bbox_inches='tight', dpi=300) # Uncomment to save the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the results as heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams[\"font.family\"] = \"Garamond\"\n",
    "\n",
    "source_titles = source_corpus[0]\n",
    "target_titles = target_corpus[0] \n",
    "\n",
    "plot_title = \"Surprise of reading texts by \" + target_label + \" (x) after texts by \" + source_label + \" (y)\"\n",
    "\n",
    "data = source_to_target_list_of_lists\n",
    "\n",
    "mask = np.zeros_like(data)\n",
    "mask[np.triu_indices_from(mask)] = False # Switch to 'True' if source corpus = target corpus, to exclude redundant data\n",
    "\n",
    "ax = sns.heatmap(data, \n",
    "                 mask=mask, \n",
    "                 cmap=\"YlGnBu\", \n",
    "                 annot=True, \n",
    "                 xticklabels=target_titles, \n",
    "                 yticklabels=source_titles, \n",
    "                 cbar=True, \n",
    "                 vmin=0.5, \n",
    "                 vmax=2).set_title(plot_title)\n",
    "\n",
    "\n",
    "#plt.savefig(\"Output/HEATMAP.png\", bbox_inches='tight', dpi=300) # Uncomment to save the file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
